##############
# @benchmark #
##############

# The @benchmark macro generates a benchmarkable function for the given `core`
# expression, then benchmarks that function, returning the ExecutionResults.
macro benchmark(core)
    name = esc(gensym())
    return quote
        begin
            Benchmarks.@benchmarkable($name, nothing, $(esc(core)), nothing)
            Benchmarks.execute($name)
        end
    end
end

##################
# @benchmarkable #
##################

# The @benchmarkable macro combines a tuple of expressions into a function
# that can be executed using `Benchmarks.execute`.
#
# Arguments:
#
#     name: The name of the function that will be generated.
#     setup: An expression that will be executed before the core
#            expression starts executing.
#     core: The core expression that will be executed.
#     teardown: An expression that will be executed after the core
#               expression finishes executing.
#
# The function generated by this macro takes three arguments:
#
#     s::Samples: A Samples object in which benchmark data will be stored
#     nsamples::Integer: The number of samples that will be gathered
#     nevals::Integer: The number of core expression evals per sample
macro benchmarkable(name, setup, core, teardown)
    # We only support function calls to be passed in `core`, but some syntaxes
    # have special parsing that will lower to a function call upon expansion
    # (e.g., A[i] or user macros). The tricky thing is that keyword functions
    # *also* have a special lowering step that we don't want (they are harder to
    # work with and would obscure some of the overhead of keyword arguments). So
    # we only expand the passed expression if it is not already a function call.
    expr = (core.head == :call) ? core : expand(core)
    expr.head == :call || throw(ArgumentError("expression to benchmark must be a function call"))
    f, fargs = expr.args[1], expr.args[2:end]
    nargs = length(expr.args)-1

    # Pull out the arguments -- both positional and keywords
    userargs = Any[] # The actual expressions the user wrote
    args = Symbol[gensym("arg_$i") for i in 1:nargs] # The local argument names
    posargs = Symbol[] # The names that are positional arguments
    kws = Expr[] # Names that are used in keyword arguments
    for i in 1:nargs
        if isa(fargs[i], Expr) && fargs[i].head == :kw
            push!(kws, Expr(:kw, fargs[i].args[1], args[i]))
            push!(userargs, fargs[i].args[2])
        else
            push!(posargs, args[i])
            push!(userargs, fargs[i])
        end
    end

    benchfn = gensym("bench")
    innerfn = gensym("inner")

    # Strategy: we create *three* functions:
    # * The outermost function is the entry point. It's simply a closure around
    #   the expressions the user passed in `core` as the arguments to the
    #   benchmarked function. This allows the arguments to be considered setup,
    #   which are evaluated in the correct scope. However, that means that
    #   within this outermost function, the arguments probably aren't
    #   concretely typed. This means that if we were to run the benchmarking
    #   function in this outermost function, we'd end up benchmarking dynamic
    #   dispatch most of the time.  So we introduce a function barrier here.
    # * The second level (`benchfn`) is the benchmarking loop.  Here is where
    #   the real work gets done.  However, if we were to call the benchmarked
    #   function directly here, it might get inlined.  And if it gets inlined,
    #   then LLVM can use optimizations that interact with the test loop itself.
    #   No longer are we simply testing the benchmarked function; we are testing
    #   the benchmark loops.  So in order to circumvent this, we introduce a
    #   third function that is explicitly marked `@noinline`
    # * It is within this third, `inner` function that we call the user's
    #   function that they want to benchmark. This means that all timings will
    #   include the overhead of at least one function call. But it also means
    #   that we can prevent LLVM from doing optimizations that are related to
    #   the benchmarking itself: it must always call the inner function in the
    #   benchmarking function (since at the mid-level it doesn't know what that
    #   function might do), and within the inner function it can only eliminate
    #   code that's unrelated to the return value (since it doesn't know what
    #   the caller might do).
    quote
        function $(esc(name))(s::Samples, nsamples, nevals)
            gc()
            $(benchfn)(s, nsamples, nevals, $(map(esc, userargs)...))
        end

        function $(benchfn)(s::Samples, nsamples, nevals, $(args...))
            # Execute the setup expression exactly once
            $(esc(setup))

            # Generate n_samples by evaluating the core
            for _ in 1:nsamples
                # Store pre-evaluation state information
                stats = Base.gc_num()
                time_before = time_ns()

                # Evaluate the core expression n_evals times.
                for _ in 1:nevals
                    out = $(innerfn)($(args...))
                end

                # get time before comparing GC info
                elapsed_time = time_ns() - time_before

                # Compare post-evaluation state with pre-evaluation state.
                diff = Base.GC_Diff(Base.gc_num(), stats)
                bytes = diff.allocd
                allocs = diff.malloc + diff.realloc + diff.poolalloc + diff.bigalloc
                gc_time = diff.total_time

                # Append data for this sample to the Samples objects.
                push!(s, nevals, elapsed_time, gc_time, bytes, allocs)
            end

            # Execute the teardown expression exactly once
            $(esc(teardown))

            # The caller receives all data via the mutated Samples object.
            return nothing
        end

        @noinline function $(innerfn)($(map(esc, args)...))
            $(esc(f))($(map(esc, posargs)...), $(map(esc, kws)...))
        end

        # "return" the outermost entry point as the final expression
        $(esc(name))
    end
end

###########
# execute #
###########

# Execute a "benchmarkable" function to estimate the time required to perform
# a single evaluation of the benchmark's core expression. To do this reliably,
# we employ a series of estimates that allow us to decide on a sampling
# strategy and an estimation strategy for performing our benchmark.
#
# Arguments:
#
#     f!::Function:        The benchmarkable function to be evaluated.
#
#     sample_limit = 100:  The max number of samples requested by the user. This
#                          limit is ignored if it is found that execution
#                          requires an OLS.
#
#     time_limit = 10:     The max number of seconds to spend benchmarking.
#
#     τ = 0.95:            The minimum R² of the OLS model before the geometric
#                          search procedure is considered to have converged.
#
#     α = 1.1:             The growth rate for the geometric search.
#
#     ols_samples = 100:   The number of samples per unique value of `n_evals`
#                          when the geometric search procedure is employed.
#
#     verbose = false:     Print progress info as we go?
#
function execute(f!::Function; verbose::Bool = false, sample_limit = 100,
                 time_limit = 10, ols_samples = 100, τ = 0.95, α = 1.1)
    start_time = time()

    # Initial benchmark of f!. Note that this can require a compilation step,
    # which might bias the resulting estimates.
    s = Samples()
    f!(s, 1, 1)

    # Stop benchmarking if we've already exhausted our time budget
    total_time = time() - start_time
    if total_time > time_limit
        return ExecutionResults(s, false, false, total_time)
    end

    # Estimate how many samples we could take before hitting the time limit.
    remaining_time_ns = (time_limit - total_time) * 10.0^9
    remaining_samples = div(remaining_time_ns, s.times[1])

    # Stop benchmarking if continuing would put us over the time limit.
    if remaining_samples < 1
        return ExecutionResults(s, false, false, total_time)
    end

    # Having reached this point, we can afford to record at least one more
    # sample without using up our time budget. The core question now is: Is the
    # expression being measured so fast that a single sample needs to evaluate
    # the core expression multiple times before a single sample can provide an
    # unbiased estimate of the expression's execution time? To determine this,
    # we execute f! one more time. This provides our first potentially unbiased
    # estimate of the execution time, because all compilations costs should now
    # have been paid.
    #
    # Before we execute f!, we empty our biased Samples object to discard
    # the values associated with our first execution of f!.
    empty!(s)

    # We evaluate f! to generate our first potentially unbiased sample.
    f!(s, 1, 1)

    # If we've used up our time or sample limits, we stop.
    total_time = time() - start_time
    if total_time > time_limit || sample_limit == 1
        return ExecutionResults(s, true, false, total_time)
    end

    # Now we determine if the function is so fast that we need to execute the
    # core expression multiple times per sample. We do this by determining if
    # the single-evaluation time is at least 1,000 times larger than the system
    # clock's resolution. If the function is at least that costly to execute,
    # then we determine how many single-evaluation samples we should employ.
    debiased_time_ns = s.times[1]
    if debiased_time_ns > 1_000 * estimate_clock_resolution()
        remaining_time_ns = (time_limit - total_time) * 10.0^9
        remaining_samples = div(remaining_time_ns, debiased_time_ns)
        f!(s, min(remaining_samples, sample_limit - 1), 1)
        return ExecutionResults(s, true, false, time() - start_time)
    end

    # If we've reached this far, we are benchmarking a function that is so fast
    # that we need to be careful with our execution strategy. In particular,
    # we need to evaluate the core expression multiple times to generate a
    # single sample. To determine the correct number of times we should
    # evaluate the core expression per sample, we perform a geometric search
    # that starts at 2 evaluations per sample and increases by a factor of 1.1
    # evaluations on each iteration. Having generated data in this form, we
    # use an OLS regression to estimate the per-evaluation timing of our core
    # expression. We stop our geometric search when the OLS linear model is
    # almost perfect fit to our empirical data.
    #
    # If verbose mode is enabled, print an info header.
    verbose && @printf("%s\t%20s\t%8s\t%s\n", "total time (s)", "n_evals", "b", "r²")

    # Now we perform a geometric search.
    finished = false
    a, b = NaN, NaN
    n_evals = 2.0 # start with 2 evals per sample
    min_time = 0.5 # spend a minimum of 0.5 seconds on our search

    while !finished
        # Gather many samples, each of which includes multiple evaluations.
        f!(s, ols_samples, ceil(Integer, n_evals))

        # Perform an OLS regression to estimate the per-evaluation time.
        a, b, r² = ols(s.evals, s.times)

        # We're finished when the OLS fit is good enough or we hit the time limit
        total_time = time() - start_time
        finished = (r² > τ && total_time > min_time) || total_time > time_limit

        # If verbose mode is enabled, print our progress
        verbose && @printf("%.1f\t%24.1f\t%12.2f\t%1.3f\n", total_time, n_evals, b, r²)

        # increase the evals per sample for the next search round
        n_evals *= α
    end

    return ExecutionResults(s, true, true, time() - start_time)
end
