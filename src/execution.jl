##############
# @benchmark #
##############

# The @benchmark macro generates a benchmarkable function for the given `core`
# expression, then benchmarks that function, returning the ExecutionResults.
macro benchmark(core)
    tmp = esc(gensym())
    return quote
        let
            $(tmp) = Benchmarks.@benchmarkable(nothing, $(esc(core)), nothing)
            Benchmarks.execute($(tmp))
        end
    end
end

##################
# @benchmarkable #
##################

# take a list of arguments and return the list with any parameter kwargs
# extracted and appended to the rest of the original arguments
function flatten_parameters(fargs)
    if !(isempty(fargs))
        arg1 = first(fargs)
        if isa(arg1, Expr) && arg1.head == :parameters
            return [drop(fargs, 1)..., arg1.args...]
        end
    end
    return fargs
end

# The @benchmarkable macro combines a tuple of expressions into a function
# that can be executed using `Benchmarks.execute`.
#
# Arguments:
#
#     setup: An expression that will be executed before the core
#            expression starts executing.
#     core: The core expression that will be executed.
#     teardown: An expression that will be executed after the core
#               expression finishes executing.
#
# The function generated by this macro takes three arguments:
#
#     s::Samples: A Samples object in which benchmark data will be stored
#     nsamples::Integer: The number of samples that will be gathered
#     nevals::Integer: The number of core expression evals per sample
macro benchmarkable(setup, core, teardown)
    # We only support function calls to be passed in `core`, but some syntaxes
    # have special parsing that will lower to a function call upon expansion
    # (e.g., A[i] or user macros). The tricky thing is that keyword functions
    # *also* have a special lowering step that we don't want (they are harder to
    # work with and would obscure some of the overhead of keyword arguments). So
    # we only expand the passed expression if it is not already a function call.
    expr = (core.head == :call) ? core : expand(core)
    expr.head == :call || throw(ArgumentError("expression to benchmark must be a function call"))
    f, fargs = expr.args[1], flatten_parameters(expr.args[2:end])
    nargs = length(expr.args)-1

    # Pull out the arguments -- both positional and keywords
    userargs = Any[] # The actual expressions the user wrote
    args = Symbol[gensym("arg_$i") for i in 1:nargs] # The local argument names
    posargs = Symbol[] # The names that are positional arguments
    kws = Expr[] # Names that are used in keyword arguments
    for i in 1:nargs
        if isa(fargs[i], Expr) && fargs[i].head == :kw
            push!(kws, Expr(:kw, fargs[i].args[1], args[i]))
            push!(userargs, fargs[i].args[2])
        else
            push!(posargs, args[i])
            push!(userargs, fargs[i])
        end
    end

    # * `benchfn`: This is the outermost "workhorse" function that is called by
    #   `execute` to collect a run of samples. This function loops over the requested
    #   number of samples, and performs the requested number of evaluations for each
    #   sample. To prevent unwanted LLVM optimizations, user arguments are reevaluated at
    #   each sample by `argsfn`, and the actual target function `f` is called via the
    #   `wrapfn` wrapper.
    #
    # * `argsfn`: This function generates the target function's user arguments every time
    #    it is called. By reevaluating the arguments for each sample, we are able to ensure
    #    that our benchmark obtains a wide range of samples for properties that can vary
    #    between allocations, like memory alignment. By forcing `argsfn` not to inline, we
    #    prevent any generated arguments from being lifted out of the sampling loop by LLVM
    #    optimizations (which would defeat the purpose of freshly evaluating the arguments
    #    for each sample).
    #
    # * `wrapfn`: This function is a wrapper around the target function `f`. Like `argsfn`,
    #   `wrapfn` exists to prevent unwanted LLVM optimizations in `benchfn`'s sampling
    #   loop. Specifically, by annotating `wrapfn` with `@noinline` will prevent LLVM from
    #   inlining `f` in `benchfn`. This also means that all timings will include a slight
    #   but consistent overhead from the extra function call.

    benchfn = gensym("bench")
    wrapfn = gensym("wrap")

    if nargs > 0
        argsfn = gensym("args")
        argsfndef = quote
            @noinline function $(argsfn)()
                return ($(map(esc, userargs)...),)
            end
        end
        argscall = :(($(args...),) = $(argsfn)())
    else # don't bother generating args if there aren't any args
        argsfndef = :()
        argscall = :()
    end

    quote
        let
            $(argsfndef)

            @noinline function $(wrapfn)($(map(esc, args)...))
                return $(esc(f))($(map(esc, posargs)...), $(map(esc, kws)...))
            end

            function $(benchfn)(samples::Benchmarks.Samples, nsamples, nevals, forcegc)
                # Execute the setup expression exactly once
                $(esc(setup))

                # Generate n_samples by evaluating the core
                for _ in 1:nsamples
                    # if forcegc is enabled, run gc() before every sample iteration
                    forcegc && gc()

                    # reevaluate the arguments at each sample
                    $(argscall)

                    # Store pre-evaluation state information
                    stats = Base.gc_num()
                    time_before = time_ns()

                    # Evaluate the core expression n_evals times.
                    for _ in 1:nevals
                        out = $(wrapfn)($(args...))
                    end

                    # get time before comparing GC info
                    elapsed_time = time_ns() - time_before

                    # Compare post-evaluation state with pre-evaluation state.
                    diff = Base.GC_Diff(Base.gc_num(), stats)
                    bytes = diff.allocd
                    allocs = diff.malloc + diff.realloc + diff.poolalloc + diff.bigalloc
                    gc_time = diff.total_time

                    # Append data for this sample to the Samples objects.
                    push!(samples, nevals, elapsed_time, gc_time, bytes, allocs)
                end

                # Execute the teardown expression exactly once
                $(esc(teardown))

                # The caller receives all data via the mutated Samples object.
                return samples
            end
        end
    end
end

###########
# execute #
###########

# Execute a "benchmarkable" function to estimate the time required to perform
# a single evaluation of the benchmark's core expression. To do this reliably,
# we employ a series of estimates that allow us to decide on a sampling
# strategy and an estimation strategy for performing our benchmark.
#
# Arguments:
#
#     f!::Function:        The benchmarkable function to be evaluated.
#
#     sample_limit = 100:  The max number of samples requested by the user. This
#                          limit is ignored if it is found that execution
#                          requires an OLS.
#
#     time_limit = 10:     The max number of seconds to spend benchmarking.
#
#     τ = 0.95:            The minimum R² of the OLS model before the geometric
#                          search procedure is considered to have converged.
#
#     α = 1.1:             The growth rate for the geometric search.
#
#     ols_samples = 100:   The number of samples per unique value of `n_evals`
#                          when the geometric search procedure is employed.
#
#     verbose = false:     Print progress info as we go?
#
#     forcegc = false:     Run gc() before/after benchmarking and between benchmark
#                          samples? Note that enabling this option can cause execution to
#                          overshoot small `time_limit`s.
#
#     disablegc = false:   Disable GC while benchmarking?

function execute(f!::Function; verbose::Bool = false, sample_limit = 100, time_limit = 10,
                 ols_samples = 100, forcegc::Bool = false, disablegc::Bool = false,
                 τ = 0.95, α = 1.1)
    @assert !(forcegc && disablegc) "disablegc and forcegc cannot be enabled simultaneously"

    forcegc && gc() # if forcegc is enabled, run gc before benchmarking
    disablegc && gc_enable(false) # if disablegc is true, turn off GC until we're finished

    start_time = time()

    # Initial benchmark of f!. Note that this can require a compilation step,
    # which might bias the resulting estimates.
    s = Samples()
    f!(s, 1, 1, forcegc)

    # Stop benchmarking if we've already exhausted our time budget
    total_time = time() - start_time
    if total_time > time_limit
        return ExecutionResults(s, false, false, total_time)
    end

    # Estimate how many samples we could take before hitting the time limit.
    remaining_time_ns = (time_limit - total_time) * 10.0^9
    remaining_samples = div(remaining_time_ns, s.times[1])

    # Stop benchmarking if continuing would put us over the time limit.
    if remaining_samples < 1
        return ExecutionResults(s, false, false, total_time)
    end

    # Having reached this point, we can afford to record at least one more
    # sample without using up our time budget. The core question now is: Is the
    # expression being measured so fast that a single sample needs to evaluate
    # the core expression multiple times before a single sample can provide an
    # unbiased estimate of the expression's execution time? To determine this,
    # we execute f! one more time. This provides our first potentially unbiased
    # estimate of the execution time, because all compilations costs should now
    # have been paid.
    #
    # Before we execute f!, we empty our biased Samples object to discard
    # the values associated with our first execution of f!.
    empty!(s)

    # We evaluate f! to generate our first potentially unbiased sample.
    f!(s, 1, 1, forcegc)

    # If we've used up our time or sample limits, we stop.
    total_time = time() - start_time
    if total_time > time_limit || sample_limit == 1
        return ExecutionResults(s, true, false, total_time)
    end

    # Now we determine if the function is so fast that we need to execute the
    # core expression multiple times per sample. We do this by determining if
    # the single-evaluation time is at least 1,000 times larger than the system
    # clock's resolution. If the function is at least that costly to execute,
    # then we determine how many single-evaluation samples we should employ.
    debiased_time_ns = s.times[1]
    if debiased_time_ns > 1_000 * estimate_clock_resolution()
        remaining_time_ns = (time_limit - total_time) * 10.0^9
        remaining_samples = div(remaining_time_ns, debiased_time_ns)
        f!(s, min(remaining_samples, sample_limit - 1), 1, forcegc)
        return ExecutionResults(s, true, false, time() - start_time)
    end

    # If we've reached this far, we are benchmarking a function that is so fast
    # that we need to be careful with our execution strategy. In particular,
    # we need to evaluate the core expression multiple times to generate a
    # single sample. To determine the correct number of times we should
    # evaluate the core expression per sample, we perform a geometric search
    # that starts at 2 evaluations per sample and increases by a factor of 1.1
    # evaluations on each iteration. Having generated data in this form, we
    # use an OLS regression to estimate the per-evaluation timing of our core
    # expression. We stop our geometric search when the OLS linear model is
    # almost perfect fit to our empirical data.
    #
    # If verbose mode is enabled, print an info header.
    verbose && @printf("%s\t%20s\t%8s\t%s\n", "total time (s)", "n_evals", "b", "r²")

    # Now we perform a geometric search.
    finished = false
    a, b = NaN, NaN
    n_evals = 2.0 # start with 2 evals per sample
    min_time = 0.5 # spend a minimum of 0.5 seconds on our search

    while !(finished)
        # Gather many samples, each of which includes multiple evaluations.
        f!(s, ols_samples, ceil(Integer, n_evals), forcegc)

        # Perform an OLS regression to estimate the per-evaluation time.
        a, b, r² = ols(s.evals, s.times)

        # We're finished when the OLS fit is good enough or we hit the time limit
        total_time = time() - start_time
        finished = (r² > τ && total_time > min_time) || total_time > time_limit

        # If verbose mode is enabled, print our progress
        verbose && @printf("%.1f\t%24.1f\t%12.2f\t%1.3f\n", total_time, n_evals, b, r²)

        # increase the evals per sample for the next search round
        n_evals *= α
    end

    results = ExecutionResults(s, true, true, time() - start_time)

    disablegc && gc_enable(true) # if GC was disabled, re-enable it now that we're finished
    forcegc && gc() # if forcegc is enabled, run once after benchmarking

    return results
end
